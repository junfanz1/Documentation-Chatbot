

<!-- TOC --><a name="documentation-chatot-"></a>
# Documentation Chatot ðŸ¤–

A sophisticated chatbot application that helps users navigate and understand LangChain documentation through an interactive, user-friendly interface.

![image](https://github.com/user-attachments/assets/ea5f488a-1a6f-4dea-8d46-e61284f5c41a)

<!-- TOC --><a name="contents"></a>
## Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [Documentation Chatot ðŸ¤–](#documentation-chatot-)
   * [Contents](#contents)
   * [1. Project Purpose](#1-project-purpose)
   * [2. Input and Output](#2-input-and-output)
   * [3. LLM Technology Stack](#3-llm-technology-stack)
   * [4. Challenges and Difficulties](#4-challenges-and-difficulties)
   * [5. Future Business Impact and Further Improvements](#5-future-business-impact-and-further-improvements)
   * [6. Target Audience and Benefits](#6-target-audience-and-benefits)
   * [7. Advantages and Disadvantages](#7-advantages-and-disadvantages)
   * [8. Tradeoffs](#8-tradeoffs)
   * [9. Highlights and Summary](#9-highlights-and-summary)
   * [10. Future Enhancements](#10-future-enhancements)
   * [11. Prerequisites](#11-prerequisites)
   * [12. Setup](#12-setup)
   * [13. Code Explanation](#13-code-explanation)
      + [`ingest_docs.py`](#ingest_docspy)
      + [`ingest_docs2.py`](#ingest_docs2py)
      + [`core.py`](#corepy)
      + [`app.py`](#apppy)
   * [14. How It Applies to the Entire Project and Each Class/Function](#14-how-it-applies-to-the-entire-project-and-each-classfunction)
   * [15. Detailed Explanation of Important Functions](#15-detailed-explanation-of-important-functions)
   * [16. Future Improvements](#16-future-improvements)
   * [17. Cursor](#17-cursor)
   * [License](#license)
   * [Acknowledgments](#acknowledgments)

<!-- TOC end -->

<!-- TOC --><a name="1-project-purpose"></a>
## 1. Project Purpose

This project aims to create a Documentation Helper Bot that utilizes Large Language Model (LLM) technology to help users quickly retrieve and understand document content. Users can ask questions, and the bot will retrieve relevant information from a specified document library and generate easy-to-understand answers. Example inputs are LangChain documents, which can also be scalable to any kind of documents in your personal or working environment.

<!-- TOC --><a name="2-input-and-output"></a>
## 2. Input and Output

* **Input**: Natural language questions asked by users.
* **Output**: Relevant information retrieved from documents, concise answers generated by LLM, and source links to related documents.

<!-- TOC --><a name="3-llm-technology-stack"></a>
## 3. LLM Technology Stack

* LangChain: Framework for building LLM applications, including document loading, text splitting, vector storage, retrieval, and question-answering chains.
* OpenAI: Used for text embeddings (text-embedding-3-small) and chat models (ChatOpenAI).
* Pinecone: Vector database for storing document embeddings.
* Streamlit: Used to build a user-friendly web interface.
* FireCrawlLoader: Used to crawl the data from websites.

<!-- TOC --><a name="4-challenges-and-difficulties"></a>
## 4. Challenges and Difficulties

* **Document Loading and Splitting**: Handling documents of different formats and structures to ensure text is correctly split into meaningful chunks.
* **Vector Database Indexing and Retrieval**: Optimizing vector database performance to improve retrieval accuracy and speed.
* **LLM Response Quality**: Ensuring that LLM-generated answers are accurate, concise, easy to understand, and provide reliable sources.
* **Website Crawling Efficiency and Accuracy**: How to efficiently crawl web pages and only crawl the main content.
* **User Interface Design**: How to design an intuitive and easy-to-use user interface that provides a good user experience.

<!-- TOC --><a name="5-future-business-impact-and-further-improvements"></a>
## 5. Future Business Impact and Further Improvements

* **Improve Work Efficiency**: Help users quickly obtain needed information, saving time and effort.
* **Enhance Knowledge Management**: Build an internal corporate knowledge base to facilitate employee information retrieval and sharing.
* **Personalized Services**: Provide personalized document assistant services based on user habits and preferences.
* **Multilingual Support**: Support documents and question answering in multiple languages.
* **Multimodal Support**: Support documents in multiple modalities such as images, audio, and video.

<!-- TOC --><a name="6-target-audience-and-benefits"></a>
## 6. Target Audience and Benefits

* **Developers**: Quickly find API documentation and code examples.
* **Students and Researchers**: Retrieve academic papers and research materials.
* **Corporate Employees**: Find internal company documents and knowledge bases.
* **General Users**: Obtain knowledge and information in various fields.

<!-- TOC --><a name="7-advantages-and-disadvantages"></a>
## 7. Advantages and Disadvantages

* **Advantages**:
    * Quick retrieval and answer generation.
    * Provide reliable document sources.
    * User-friendly interface.
* **Disadvantages**:
    * Relies on LLM performance and accuracy.
    * May not provide satisfactory answers for complex or in-depth questions.
    * When crawling websites, crawling may fail due to changes in website structure.

<!-- TOC --><a name="8-tradeoffs"></a>
## 8. Tradeoffs

* **Accuracy and Speed**: Try to improve response speed while ensuring retrieval accuracy.
* **Cost and Performance**: Choose appropriate LLMs and vector databases to balance cost and performance.
* **User Experience and Functionality**: While providing rich functionality, ensure the user interface is simple and easy to use.

<!-- TOC --><a name="9-highlights-and-summary"></a>
## 9. Highlights and Summary

This project uses advanced technologies such as LangChain, OpenAI, and Pinecone to build an efficient Documentation Helper Bot. Through natural language question answering, users can quickly obtain relevant information from documents, improving work efficiency and knowledge acquisition capabilities.

<!-- TOC --><a name="10-future-enhancements"></a>
## 10. Future Enhancements

* Support more types of documents and data sources.
* Optimize the quality and accuracy of LLM responses.
* Enhance user interface interactivity and personalization.
* Add user feedback and rating mechanisms.
* Support multi-turn conversations and contextual understanding.

<!-- TOC --><a name="11-prerequisites"></a>
## 11. Prerequisites

* Python 3.7+
* OpenAI API key
* Pinecone API key
* Install required Python packages (see requirements.txt)

<!-- TOC --><a name="12-setup"></a>
## 12. Setup

1.  Clone the repository:

    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

3.  Set environment variables:

    Create a `.env` file and add OpenAI and Pinecone API keys:

    ```
    OPENAI_API_KEY=<your_openai_api_key>
    PINECONE_API_KEY=<your_pinecone_api_key>
    PINECONE_API_ENV=<your_pinecone_api_env>
    ```

4. Run the ingest\_docs2.py file, which will crawl the web and store the data in the pinecone database.

    ```bash
    python ingest_docs2.py
    ```

5.  Run the Streamlit app:

    ```bash
    streamlit run app.py
    ```

![image](https://github.com/user-attachments/assets/57894307-798b-443c-9a2f-2ec851d032e0)


<!-- TOC --><a name="13-code-explanation"></a>
## 13. Code Explanation

<!-- TOC --><a name="ingest_docspy"></a>
### `ingest_docs.py`

* **Function**: Loads documents from the specified document library (ReadTheDocs), splits the text, and stores the document embeddings in the Pinecone vector database.
* **Functions**:
    * `ingest_docs()`: Load documents, split text, and vectorize the text into the pinecone database.
* **Code**:
    * Use `ReadTheDocsLoader` to load documents.
    * Use `RecursiveCharacterTextSplitter` to split text.
    * Use `OpenAIEmbeddings` to generate text embeddings.
    * Use `PineconeVectorStore` to store document embeddings.

<!-- TOC --><a name="ingest_docs2py"></a>
### `ingest_docs2.py`

* **Function**: Crawl data from the specified website, split the text, and store the document embeddings in the Pinecone vector database.
* **Functions**:
    * `ingest_docs2()`: Crawl web pages, load documents, split text, and vectorize the text into the pinecone database.
* **Code**:
    * Use `FireCrawlLoader` to crawl the web.
    * Use `RecursiveCharacterTextSplitter` to split text.
    * Use `OpenAIEmbeddings` to generate text embeddings.
    * Use `PineconeVectorStore` to store document embeddings.

<!-- TOC --><a name="corepy"></a>
### `core.py`

* **Function**: Define the LLM question-answering chain, process user queries, and return answers and document sources.
* **Functions**:
    * `run_llm(query, chat_history)`: Process user queries and return answers and document sources.
* **Code**:
    * Use `OpenAIEmbeddings` to generate query embeddings.
    * Use `PineconeVectorStore` to retrieve relevant documents from the vector database.
    * Use `ChatOpenAI` and LangChain chains to generate answers.
    * Return answers and document sources.

<!-- TOC --><a name="apppy"></a>
### `app.py`

* **Function**: Use Streamlit to build a user interface, receive user input, and display LLM-generated answers.
* **Code**:
    * Use Streamlit's `text_input` to receive user queries.
    * Call `core.run_llm` to process queries.
    * Use Streamlit's `write` to display answers and document sources.
    * Use streamlit's session\_state to save chat history.
    * Use CSS styling to beautify the interface.

<!-- TOC --><a name="14-how-it-applies-to-the-entire-project-and-each-classfunction"></a>
## 14. How It Applies to the Entire Project and Each Class/Function

* `ingest_docs2.py` is responsible for crawling data from websites and storing the vectorized data into the pinecone database, providing data support for the Documentation Helper Bot.
* `core.py` is responsible for processing user queries, retrieving relevant documents from the vector database, and generating answers. It is the core logic of the Documentation Helper Bot.
* `app.py` is responsible for building the user interface, receiving user input, and displaying LLM-generated answers. It is the user interaction part of the Documentation Helper Bot.

<!-- TOC --><a name="15-detailed-explanation-of-important-functions"></a>
## 15. Detailed Explanation of Important Functions

* `core.run_llm(query, chat_history)`:
    * This function is the core of the LLM question-answering chain, responsible for processing user queries and generating answers.
    * It first uses `OpenAIEmbeddings` to generate query embeddings, and then uses `PineconeVectorStore` to retrieve relevant documents from the vector database.
    * Next, it uses LangChain's `create_history_aware_retriever` and `create_retrieval_chain` to build a question-answering chain, and uses `ChatOpenAI` to generate answers.
    * Finally, it returns answers and document sources.

<!-- TOC --><a name="16-future-improvements"></a>
## 16. Future Improvements

* Optimize the indexing and retrieval performance of the vector database.
* Improve the accuracy and relevance of LLM-generated answers.
* Add user authentication and permission management.
* Add more ways to visualize the data.
* Improve the overall look and feel of the user interface


<!-- TOC --><a name="17-cursor"></a>
## 17. Cursor
With the polish of Cursor:
![image](https://github.com/user-attachments/assets/9b03160a-451b-42a5-a4e5-b96870312dd1)
![image](https://github.com/user-attachments/assets/6357212a-ff32-4111-b529-4ef8df2e1d31)

README generation prompt:

```
According to this project and all the coding files you have, generate a Github Readme for me, including: (1) purpose of the project, (2) input and output, (3) LLM Technology Stack, (4) Challenges and Difficulties, (5) Future Business Impact and Further Improvements, (6) Target Audience and Benefits, (7) Advantages and Disadvantages, (8) Tradeoffs, (9) Highlight and Summary, (10) Future Enhancements, then for the functionality to run my project, provide (11) Prerequisites, (12) Setup, (13) Code Explanation for each file and each function, (14) How it works for the whole project and each class/function, (15) Any function you think is crucial for handling the project make it detailed elaboration, (16) Future Improvements, (17) Anything else you think is important to add in this readme. Finally, generate the readme in markdown format
```

<!-- TOC --><a name="license"></a>
## License
This project is licensed under the MIT License - see the LICENSE file for details.

<!-- TOC --><a name="acknowledgments"></a>
## Acknowledgments

[Eden Marco: LangChain- Develop LLM powered applications with LangChain](https://www.udemy.com/course/langchain/?srsltid=AfmBOooPg0Xkc19q5W1430Dzq6MHGKWqHtq5a1WY4uUl9sQkrh_b_pej&couponCode=ST4MT240225B)









